{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659d2438-8fd6-456e-b620-3f7bef23e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris X shape: (150, 4)\n",
      "Iris y shape: (150,)\n",
      "Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Blobs X shape: (100, 2)\n",
      "Blobs y shape: (100,)\n",
      "Blobs features: ['feature_1', 'feature_2']\n",
      "Customer data columns: ['age', 'annual_income', 'spending_score', 'purchase_frequency']\n",
      "First 5 rows:\n",
      "          age  annual_income  spending_score  purchase_frequency\n",
      "0  38.656605   55782.387974              46                   3\n",
      "1  22.520191   62625.764517              27                   4\n",
      "2  44.005414   20856.883603              77                   6\n",
      "3  46.286777   43606.575673              97                   5\n",
      "4  11.587578   40592.546914              27                   3\n"
     ]
    }
   ],
   "source": [
    "from clusterviz import datasets\n",
    "\n",
    "# Loading the Iris dataset, which is a classic dataset in machine learning.\n",
    "# It contains 150 samples of iris flowers with 4 features each\n",
    "X, y, features = datasets.load_iris()\n",
    "print(\"Iris X shape:\", X.shape)    \n",
    "print(\"Iris y shape:\", y.shape)    \n",
    "print(\"Features:\", features)       \n",
    "\n",
    " \n",
    "# A \"blobs\" dataset is a synthetic dataset used for clustering experiments.\n",
    "# Points are generated around specified centers, forming groups (clusters).\n",
    "\n",
    "Xblob, yblob, fblob = datasets.make_blobs_dataset(n_samples=100, centers=3, n_features=2)\n",
    "print(\"Blobs X shape:\", Xblob.shape)   \n",
    "print(\"Blobs y shape:\", yblob.shape)  # Cluster labels (0,1,2)\n",
    "print(\"Blobs features:\", fblob)       \n",
    "\n",
    "# 3. Loading a synthetic customer data\n",
    "\n",
    "df = datasets.load_customer_data(n_customers=50)\n",
    "print(\"Customer data columns:\", df.columns.tolist())\n",
    "print(\"First 5 rows:\\n\", df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4e1aa4-60b9-4779-8f2f-5b91cd34b8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (5, 3)\n",
      "Transformed shape: (5, 2)\n",
      "Explained variance ratio: [1.00000000e+00 9.86076132e-33]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from clusterviz.preprocessing import Preprocessor\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"age\": [20, 25, 30, 35, 40],\n",
    "    \"income\": [30000, 35000, 40000, 45000, 50000],\n",
    "    \"spending\": [100, 200, 300, 400, 500]\n",
    "})\n",
    "\n",
    "pre = Preprocessor()\n",
    "X_trans = pre.fit_transform(df, scale=True, pca_components=2)\n",
    "\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(\"Transformed shape:\", X_trans.shape)\n",
    "print(\"Explained variance ratio:\", pre.get_explained_variance_ratio())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b284f0-4f4d-49a6-b0b4-be80e8927459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data shape: (150, 4)\n",
      "Mean of each feature (should be ~0): [-1.69031455e-15 -1.84297022e-15 -1.69864123e-15 -1.40924309e-15]\n",
      "Std of each feature (should be ~1): [1. 1. 1. 1.]\n",
      "PCA transformed shape: (150, 2)\n",
      "Explained variance ratio: [0.72962445 0.22850762]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.264703</td>\n",
       "      <td>0.480027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.080961</td>\n",
       "      <td>-0.674134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.364229</td>\n",
       "      <td>-0.341908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.299384</td>\n",
       "      <td>-0.597395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.389842</td>\n",
       "      <td>0.646835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2\n",
       "0 -2.264703  0.480027\n",
       "1 -2.080961 -0.674134\n",
       "2 -2.364229 -0.341908\n",
       "3 -2.299384 -0.597395\n",
       "4 -2.389842  0.646835"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from clusterviz.preprocessing import Preprocessor\n",
    "\n",
    "# Creating Preprocessor object\n",
    "pre = Preprocessor()\n",
    "# Fitting and transforming the data (scaling only, no PCA)\n",
    "# Scaling is very crucial for distance-based clustering algorithms \n",
    "# (like DBSCAN or k-means) that we are going to perform later, to ensure \n",
    "# that no feature dominates because of its scale.\n",
    "X_scaled = pre.fit_transform(X, scale=True, pca_components=None)\n",
    "\n",
    "# Checking the transformed data\n",
    "print(\"Scaled data shape:\", X_scaled.shape)\n",
    "print(\"Mean of each feature (should be around 0):\", X_scaled.mean(axis=0))\n",
    "print(\"Std of each feature (should be around 1):\", X_scaled.std(axis=0))\n",
    "\n",
    "\n",
    "# PCA finds new axes (principal components) capturing the most variance.\n",
    "# Here, we reduce from 4 features to 2 components for easy visualization and faster clustering.\n",
    "# explained_variance_ratio function tells how much of the original variance each principal \n",
    "# component captures.\n",
    "# Fitting and transforming data with PCA to reduce to 2 components\n",
    "X_pca = pre.fit_transform(X, scale=True, pca_components=2)\n",
    "\n",
    "# Checking the PCA output\n",
    "print(\"PCA transformed shape:\", X_pca.shape)\n",
    "print(\"Explained variance ratio:\", pre.get_explained_variance_ratio())\n",
    "\n",
    "# Visualizing the first few rows\n",
    "pd.DataFrame(X_pca, columns=[\"PC1\", \"PC2\"]).head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb7065-b4b7-43cc-b84e-7600afeae0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC1 captures around 73% of the variance, PC2 captures around 23%. Together they capture 96% of the \n",
    "# total variance. Most of the original information is retained with fewer dimensions, which speeds up \n",
    "# clustering and helps visualization.\n",
    "\n",
    "# Aboove are printed the first 5 rows of the PCA-transformed data.\n",
    "# Each row is now in the principal component space instead of original feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8e8689c-83e6-44a8-9f89-a5fbe8e52d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed new data shape: (5, 2)\n"
     ]
    }
   ],
   "source": [
    "# Supposing we have now a new data (test or additional samples)\n",
    "X_new = X[:5, :]  # just taking first 5 samples as example\n",
    "\n",
    "# Then we transform them using the already fitted scaler + PCA\n",
    "# The transform() function is used on new/unseen data to ensure consistent preprocessing\n",
    "# with training data.\n",
    "\n",
    "X_new_transformed = pre.transform(X_new)\n",
    "print(\"Transformed new data shape:\", X_new_transformed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049ea6a-194c-4b2e-be85-64bacd01d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The new/unseen data are now transformed using the fitted scaler + PCA.\n",
    "# This ensures consistency in preprocessing when applying clustering to new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452864d-5c3a-4fda-8e83-3b50c469c29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "310 XGBoost",
   "language": "python",
   "name": "py310_xgb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
